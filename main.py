
import time
import cv2
import numpy as np
import mediapipe as mp
# for visualizing results
from mediapipe import solutions
from mediapipe.framework.formats import landmark_pb2

result_landmarks = mp.tasks.vision.HandLandmarkerResult

def createLandmarker():
   # landmarker object
   landmarker = mp.tasks.vision.HandLandmarker

   # callback function options
   def print_result(result: mp.tasks.vision.HandLandmarkerResult, output_image: mp.Image, timestamp_ms: int):
      print('hand landmarker result: {}'.format(result))

   def update_result(result: mp.tasks.vision.HandLandmarkerResult, output_image: mp.Image, timestamp_ms: int):
      global result_landmarks
      result_landmarks = result
   
   # HandLandmarkerOptions (details here: https://developers.google.com/mediapipe/solutions/vision/hand_landmarker/python#live-stream)
   options = mp.tasks.vision.HandLandmarkerOptions( 
      base_options = mp.tasks.BaseOptions(model_asset_path="hand_landmarker.task"), # path to model
      running_mode = mp.tasks.vision.RunningMode.LIVE_STREAM, # running on a live stream
      num_hands = 2, # track both hands
      min_hand_detection_confidence = 0.3, # lower than value to get predictions more often
      min_hand_presence_confidence = 0.3, # lower than value to get predictions more often
      min_tracking_confidence = 0.3, # lower than value to get predictions more often
      result_callback=update_result)
   
   # create and return landmarker object
   return landmarker.create_from_options(options)

def draw_landmarks_on_image(rgb_image, detection_result: mp.tasks.vision.HandLandmarkerResult):
   """Courtesy of https://github.com/googlesamples/mediapipe/blob/main/examples/hand_landmarker/python/hand_landmarker.ipynb"""
   try:
      if detection_result.hand_landmarks == []:
         return rgb_image
      else:
         hand_landmarks_list = detection_result.hand_landmarks
         handedness_list = detection_result.handedness
         annotated_image = np.copy(rgb_image)

         # Loop through the detected hands to visualize.
         for idx in range(len(hand_landmarks_list)):
            hand_landmarks = hand_landmarks_list[idx]
            handedness = handedness_list[idx]

            # Draw the hand landmarks.
            hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()
            hand_landmarks_proto.landmark.extend([
               landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks])
            solutions.drawing_utils.draw_landmarks(
               annotated_image,
               hand_landmarks_proto,
               solutions.hands.HAND_CONNECTIONS,
               solutions.drawing_styles.get_default_hand_landmarks_style(),
               solutions.drawing_styles.get_default_hand_connections_style())

         return annotated_image
   except:
      return rgb_image

def count_fingers_raised(rgb_image, detection_result: mp.tasks.vision.HandLandmarkerResult):
   """Iterate through each hand, checking if fingers (and thumb) are raised.
   Hand landmark enumeration (and weird naming convention) comes from
   https://developers.google.com/mediapipe/solutions/vision/hand_landmarker."""
   try:
      # Get Data
      hand_landmarks_list = detection_result.hand_landmarks
      # Counter
      numRaised = 0
      # for each hand...
      for idx in range(len(hand_landmarks_list)):
         # hand landmarks is a list of landmarks where each entry in the list has an x, y, and z in normalized image coordinates
         hand_landmarks = hand_landmarks_list[idx]
         # for each fingertip... (hand_landmarks 4, 8, 12, and 16)
         for i in range(8,21,4):
            # make sure finger is higher in image the 3 proceeding values (2 finger segments and knuckle)
            tip_y = hand_landmarks[i].y
            dip_y = hand_landmarks[i-1].y
            pip_y = hand_landmarks[i-2].y
            mcp_y = hand_landmarks[i-3].y
            if tip_y < min(dip_y,pip_y,mcp_y):
               numRaised += 1
         # for the thumb
         # use direction vector from wrist to base of thumb to determine "raised"
         tip_x = hand_landmarks[4].x
         dip_x = hand_landmarks[3].x
         pip_x = hand_landmarks[2].x
         mcp_x = hand_landmarks[1].x
         palm_x = hand_landmarks[0].x
         if mcp_x > palm_x:
            if tip_x > max(dip_x,pip_x,mcp_x):
               numRaised += 1
         else:
            if tip_x < min(dip_x,pip_x,mcp_x):
               numRaised += 1
         
         
      # display number of fingers raised on the image
      annotated_image = np.copy(rgb_image)
      height, width, _ = annotated_image.shape
      text_x = int(hand_landmarks[0].x * width) - 100
      text_y = int(hand_landmarks[0].y * height) + 50
      cv2.putText(img = annotated_image, text = str(numRaised) + " Fingers Raised",
                        org = (text_x, text_y), fontFace = cv2.FONT_HERSHEY_DUPLEX,
                        fontScale = 1, color = (0,0,255), thickness = 2, lineType = cv2.LINE_4)
      return annotated_image
   except:
      return rgb_image

def main():
   # access webcam
   cap = cv2.VideoCapture(0)

   # create landmarker
   hand_landmarker = createLandmarker()

   # access global result variable
   global result_landmarks

   # open landmarker and stream data
   with hand_landmarker as landmarker:
      while True:
         ret, frame = cap.read()
         frame = cv2.flip(frame, 1)
         # convert frame to mp image file
         mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=frame)
         # process image
         landmarker.detect_async(image = mp_image, timestamp_ms = int(time.time() * 1000))
         # draw landmarks on frame
         frame = draw_landmarks_on_image(frame,result_landmarks)
         # count number of fingers raised
         frame = count_fingers_raised(frame, result_landmarks)

         cv2.imshow('frame',frame)
         if cv2.waitKey(1) == ord('q'):
            break
   
   # release everything
   cap.release()
   cv2.destroyAllWindows()

if __name__ == "__main__":
   main()
